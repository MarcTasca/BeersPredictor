{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyaU8DdcL-Dv"
      },
      "source": [
        "! wget https://github.com/facebookresearch/fastText/archive/v0.9.2.zip\n",
        "! unzip v0.9.2.zip\n",
        "# for python bindings :\n",
        "! pip install /content/fastText-0.9.2\n",
        "import fasttext\n",
        "\n",
        "! pip install optuna\n",
        "import optuna\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import string\n",
        "import re\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.kernel_approximation import Nystroem\n",
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from matplotlib.legend_handler import HandlerLine2D\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdaUuSGtPcB7"
      },
      "source": [
        "# ---------- USEFUL FUNCTIONS ----------\n",
        "\n",
        "# write the csv to submit on the platform\n",
        "def write_submission(x, y, name):\n",
        "  result = pd.DataFrame()\n",
        "  result['Id'] = x.index\n",
        "  result['Predicted'] = y\n",
        "  result.to_csv(os.path.join(data, name), index=False, header=True, sep=',')\n",
        "\n",
        "# stemming every words from dataset's reviews\n",
        "def stemming(review):\n",
        "  words = word_tokenize(review)\n",
        "  ps = PorterStemmer()\n",
        "  return \" \".join([ps.stem(w) for w in words])\n",
        "\n",
        "# preprocess the dataset's reviews, except for stemmization\n",
        "def preprocessing_text(input_str):\n",
        "  # removing numbers\n",
        "  input_str = re.sub(r'\\d+', '', input_str)\n",
        "  # to lower\n",
        "  input_str = input_str.lower()\n",
        "  # removing puntuaction\n",
        "  input_str = input_str.translate(str.maketrans('', '', string.punctuation))\n",
        "  # remove white spaces\n",
        "  input_str = \" \".join(input_str.split())\n",
        "  # stemming the review\n",
        "  # input_str = stemming(input_str)\n",
        "  return input_str\n",
        "\n",
        "# fasttext unsupervised training -> my_little_mushroom.bin\n",
        "# it takes 16 minutes straight\n",
        "def train_fasttext(df):\n",
        "  feed = open(r\"food_for_fasttext.txt\",\"w\")\n",
        "  feed.write(''.join(df['review/text'].values))\n",
        "  feed.close()\n",
        "  model = fasttext.train_unsupervised('food_for_fasttext.txt', dim=150)\n",
        "  model.save_model('/content/drive/MyDrive/progetto dsl/fasttext_model/my_little_mushroom.bin')\n",
        "  return model, 'my_little_mushroom.bin'\n",
        "\n",
        "# fasttext sentence vectorizer on dataset's reviews\n",
        "def text_transformation_fasttext(df, model):\n",
        "  ft = df['review/text'].apply(model.get_sentence_vector).apply(pd.Series)\n",
        "  df_ft = pd.concat([df, ft], axis=1)\n",
        "  return df_ft\n",
        "\n",
        "# countvectorizer + svd\n",
        "# it could also be tfidf\n",
        "# you just need to add .toarray() to the matrix and to comment and uncomment two lines of code\n",
        "def text_transform(train, test, mode):\n",
        "  #model = TfidfVectorizer(max_df=0.9, min_df=0.1)\n",
        "  model1 = CountVectorizer()\n",
        "  model2 = TruncatedSVD(n_components=150)\n",
        "  model = Pipeline([('countvect',model1),('svd',model2)])\n",
        "  # train\n",
        "  train_matrix = model.fit_transform(train['review/text'])\n",
        "  train_ft = pd.concat([train, pd.DataFrame(train_matrix, index=train.index)], axis=1)\n",
        "\n",
        "  # test\n",
        "  test_matrix = model.transform(test['review/text'])\n",
        "  test_ft = pd.concat([test, pd.DataFrame(test_matrix, index=test.index)], axis=1)\n",
        "\n",
        "  return train_ft, test_ft\n",
        "\n",
        "# one hot encoder for beer_style return train and test\n",
        "def ohe_beer(train_ft, test_ft, mode):  \n",
        "  model = OneHotEncoder(handle_unknown='ignore')\n",
        "  #model2 = TruncatedSVD(n_components=15)\n",
        "  #model = Pipeline([('onehot',model1),('pca',model2)])\n",
        "\n",
        "  # train\n",
        "  beer_style = train_ft[['beer/style']]\n",
        "  encoded_matrix = model.fit_transform(beer_style)\n",
        "  train_ft_b = pd.concat([train_ft, pd.DataFrame(encoded_matrix.toarray(), index=beer_style.index)], axis=1)\n",
        "\n",
        "  # test\n",
        "  beer_style = test_ft[['beer/style']]\n",
        "  encoded_matrix = model.transform(beer_style)\n",
        "  test_ft_b = pd.concat([test_ft, pd.DataFrame(encoded_matrix.toarray(), index=beer_style.index)], axis=1)\n",
        "\n",
        "  return train_ft_b, test_ft_b\n",
        "\n",
        "# drop unused columns prepare and return x and y\n",
        "def drop_prepare(train_ft_b, test_ft_b, mode):\n",
        "  x_train = train_ft_b.drop(columns=['review/overall','review/text','beer/style'])\n",
        "  if mode == 'development':\n",
        "    x_test = test_ft_b.drop(columns=['review/overall','review/text','beer/style'])\n",
        "  elif mode == 'evaluation':\n",
        "    x_test = test_ft_b.drop(columns=['review/text','beer/style'])\n",
        "\n",
        "  y_train = train_ft_b['review/overall']\n",
        "  if mode == 'development':\n",
        "    y_test = test_ft_b['review/overall']\n",
        "  else: y_test = None\n",
        "\n",
        "  return x_train, y_train, x_test, y_test\n",
        "\n",
        "# return the score of the selected model\n",
        "def score_model(x_train, y_train, x_test, y_test, model):\n",
        "  model.fit(x_train,y_train)\n",
        "  y_pred = model.predict(x_test)\n",
        "  \n",
        "  return r2_score(y_pred,y_test)\n",
        "\n",
        "# train the model, return the prediction and groung truth\n",
        "def train_model(x_train, y_train, x_test, model):\n",
        "  model.fit(x_train,y_train)\n",
        "  y_pred = model.predict(x_test)\n",
        "  return x_test, y_pred\n",
        "\n",
        "# tuning with graphs the hyperparameters of rigde with polynomial_features\n",
        "def model_score(x_train, y_train, x_test, y_test, models):\n",
        "  # r2_score results\n",
        "  train_results = []\n",
        "  test_results = []\n",
        "\n",
        "  for model in models:\n",
        "    print('-', end='')\n",
        "    # train the model\n",
        "    model.fit(x_train, y_train)\n",
        "\n",
        "    # r2_score for training set\n",
        "    train_pred = model.predict(x_train)\n",
        "    out = r2_score(y_train, train_pred)\n",
        "    train_results.append(out)\n",
        "\n",
        "    # r2_score for test set\n",
        "    y_pred = model.predict(x_test)\n",
        "    out = r2_score(y_test, y_pred)\n",
        "    test_results.append(out)\n",
        "\n",
        "  return train_results, test_results\n",
        "\n",
        "# plot the results\n",
        "def plot_r2score(tuning_param, xlabel, train_results, test_results):\n",
        "  line1, = plt.plot(tuning_param, train_results, 'b', label='Train R2')\n",
        "  line2, = plt.plot(tuning_param, test_results, 'r', label='Test R2')\n",
        "  plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
        "  plt.ylabel('R2 score')\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.show()\n",
        "\n",
        "# manual cross validation returning r2_score\n",
        "def manual_cross_validation(df, n, model, mode):\n",
        "  kfold = KFold(n_splits=n, random_state=21, shuffle=True)\n",
        "  score = 0\n",
        "\n",
        "  for train_id, test_id in kfold.split(df):\n",
        "    # counting fold\n",
        "    # print('| ',end='')\n",
        "\n",
        "    # split train and test\n",
        "    train_ft = df.iloc[train_id]\n",
        "    test_ft = df.iloc[test_id]\n",
        "\n",
        "    # train_ft, test_ft = text_transform(train, test, mode)\n",
        "    train_ft_b, test_ft_b = ohe_beer(train_ft, test_ft, mode)\n",
        "    x_train, y_train, x_test, y_test = drop_prepare(train_ft_b, test_ft_b, mode)\n",
        "\n",
        "    score += score_model(x_train, y_train, x_test, y_test, model)\n",
        "  return score/n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NDezK-vfA-U"
      },
      "source": [
        "# ---------- PREPROCESSING DATASET ----------\n",
        "\n",
        "# retrieve and preliminary steps to df and df_eval\n",
        "data = '/content/drive/MyDrive/progetto dsl/data/student_files'\n",
        "\n",
        "df = pd.read_table(os.path.join(data,'development.tsv'))\n",
        "df_eval = pd.read_table(os.path.join(data,'evaluation.tsv'))\n",
        "\n",
        "# remove useless features (df.isna().sum()) with a lot of NaN values\n",
        "#user/ageInSeconds    55355\n",
        "#user/birthdayRaw     55355\n",
        "#user/birthdayUnix    55355\n",
        "#user/gender          41819\n",
        "# user/pofileName is not useful, because it has too many categorical values\n",
        "# beer/name: too many categorical values\n",
        "df.drop(columns=['user/ageInSeconds', 'user/birthdayRaw', 'user/birthdayUnix', 'user/gender','user/profileName','beer/name'], inplace=True)\n",
        "df_eval.drop(columns=['user/ageInSeconds', 'user/birthdayRaw', 'user/birthdayUnix', 'user/gender','user/profileName','beer/name'], inplace=True)\n",
        "\n",
        "# fill nan values of ABV with the mean of the column\n",
        "mean = df['beer/ABV'].mean()\n",
        "df['beer/ABV'].fillna(mean, inplace=True) # development df\n",
        "df_eval['beer/ABV'].fillna(mean, inplace=True) # evaluation df\n",
        "\n",
        "# drop the entire row if review/text is null, development df\n",
        "# mask = df['review/text'].isna() == False\n",
        "# df = df[mask]\n",
        "df['review/text'].fillna('', inplace=True)\n",
        "\n",
        "#fill nan values in the review/text column, evaluation df\n",
        "df_eval['review/text'].fillna('', inplace=True)\n",
        "\n",
        "# preprocess text in df and df_eval\n",
        "df['review/text'] = df['review/text'].apply(preprocessing_text)\n",
        "df_eval['review/text'] = df_eval['review/text'].apply(preprocessing_text)\n",
        "\n",
        "# using FASTTEXT: ---------------\n",
        "\n",
        "# here, if the model is not trained, we can call this function (16:00 minutes to train)\n",
        "# model_df, name_of_model = train_fasttext(df)\n",
        "\n",
        "# if the fasttext model is trained, we simply load it\n",
        "print('model loading...')\n",
        "model_ft = fasttext.load_model('/content/drive/MyDrive/progetto dsl/fasttext_model/my_little_mushroom.bin')\n",
        "print('done!')\n",
        "\n",
        "# then we can convert the reviews to vectors (1:50 minutes)\n",
        "print('start sentence vectoring...')\n",
        "df_ft = text_transformation_fasttext(df, model_ft)\n",
        "df_eval_ft = text_transformation_fasttext(df_eval, model_ft)\n",
        "print('done!')\n",
        "\n",
        "# we can now choose between (df, df_eval) and the versions with fasttext's sentence vectors (df_ft, df_eval_ft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ivE7e2xFPVu"
      },
      "source": [
        "# ---------- THIS IS THE FINAL MODEL ----------\n",
        "\n",
        "# dataframe_of_params_optuna = study.trials_dataframe()\n",
        "# dataframe_of_params_optuna.to_csv('/content/drive/MyDrive/progetto dsl/optuna/optuna_dataframe.csv')\n",
        "# parameters = study.best_params\n",
        "\n",
        "parameters = {'loss': 'least_squares', \n",
        "              'learning_rate': 0.0918570824629043, \n",
        "              'max_iter': 188, \n",
        "              'max_leaf_nodes': 42, \n",
        "              'min_samples_leaf': 97, \n",
        "              'l2_regularization': 0.9441432740957117, \n",
        "              'n_iter_no_change': 18, \n",
        "              'tol': 8.076264931952568e-07}\n",
        "\n",
        "# let's cross fingers and light it up with optuna\n",
        "mode = 'evaluation'\n",
        "train_ft = df_ft\n",
        "test_ft = df_eval_ft\n",
        "train_ft_b, test_ft_b = ohe_beer(train_ft, test_ft, mode)\n",
        "x_train, y_train, x_test, _ = drop_prepare(train_ft_b, test_ft_b, mode)\n",
        "\n",
        "name = 'optunadHist.csv'\n",
        "model = HistGradientBoostingRegressor(**parameters)\n",
        "x, y = train_model(x_train, y_train, x_test, model)\n",
        "write_submission(x, y, name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL_XI_kyuUHV"
      },
      "source": [
        "# auto-tuning with Optuna\n",
        "model = HistGradientBoostingRegressor()\n",
        "n = 10\n",
        "mode = 'development'\n",
        "\n",
        "def objective(trial):    \n",
        "\n",
        "  loss = trial.suggest_categorical('loss', ['least_squares', 'least_absolute_deviation'])\n",
        "  learning_rate = trial.suggest_uniform('learning_rate', 0, 1)\n",
        "  max_iter = trial.suggest_int('max_iter', 1, 200)\n",
        "  max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 2, 100)\n",
        "  min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 100)\n",
        "  l2_regularization = trial.suggest_uniform('l2_regularization', 0, 1)\n",
        "  n_iter_no_change = trial.suggest_int('n_iter_no_change', 1, 20)\n",
        "  tol = trial.suggest_loguniform('tol', 1e-10, 1)\n",
        "\n",
        "  params = {\n",
        "    'loss': loss,\n",
        "    'learning_rate': learning_rate,\n",
        "    'max_iter': max_iter,\n",
        "    'max_leaf_nodes': max_leaf_nodes,\n",
        "    'min_samples_leaf': min_samples_leaf,\n",
        "    'l2_regularization': l2_regularization,\n",
        "    'n_iter_no_change': n_iter_no_change,\n",
        "    'tol': tol,\n",
        "  }\n",
        "\n",
        "  model.set_params(**params)\n",
        "  return - np.mean(manual_cross_validation(df_ft, n, model, mode))\n",
        "\n",
        "\n",
        "study = optuna.create_study()\n",
        "study.optimize(objective, timeout=4*3600)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjG_AlvdqfAC"
      },
      "source": [
        "# preliminary investigation\n",
        "dieffe = pd.read_table(os.path.join(data,'development.tsv'))\n",
        "# missing values\n",
        "print(dieffe.isna().sum())\n",
        "\n",
        "#unique values\n",
        "for column in dieffe.columns:\n",
        "  print(column, ': ', dieffe[column].value_counts().count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Thl__zb0sDKW"
      },
      "source": [
        "# it is not a completely normal distribution\n",
        "plt.hist(df['review/overall'], bins=9)\n",
        "plt.xlabel('overall score')\n",
        "plt.ylabel('reviews')\n",
        "plt.show()\n",
        "\n",
        "# skew\n",
        "print(df['review/overall'].skew())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjf5ndm_nX5A"
      },
      "source": [
        "# evaluation of the models\n",
        "# some are not used in our report\n",
        "# I have left them here just for fun\n",
        "\n",
        "mode = 'development'\n",
        "n = 5\n",
        "\n",
        "pipelines = []\n",
        "#pipelines.append(('ScaledLinearSVR', Pipeline([('Scaler', StandardScaler()),('Nystroem',Nystroem()),('linearSVR',LinearSVR())])))\n",
        "#pipelines.append(('ScaledSGD', Pipeline([('Scaler', StandardScaler()),('Nystroem',Nystroem()),('SGD',SGDRegressor())])))\n",
        "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR',LinearRegression())])))\n",
        "pipelines.append(('ScaledLASSO', Pipeline([('Scaler', StandardScaler()),('LASSO', Lasso())])))\n",
        "#pipelines.append(('ScaledEN', Pipeline([('Scaler', StandardScaler()),('EN', ElasticNet())])))\n",
        "pipelines.append(('ScaledRIDGE', Pipeline([('Scaler', StandardScaler()),('RIDGE', Ridge())])))\n",
        "pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeRegressor())])))\n",
        "#pipelines.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingRegressor())])))\n",
        "pipelines.append(('ScaledHistGBM', Pipeline([('Scaler', StandardScaler()),('HistGBM', HistGradientBoostingRegressor())])))\n",
        "#pipelines.append(('scaledHistGBMtuned', Pipeline([('Scaler', StandardScaler()), ('HistGBM', HistGradientBoostingRegressor(**parameters))])))\n",
        "#pipelines.append(('HistGBMtuned', Pipeline([('HistGBM', HistGradientBoostingRegressor(**parameters))])))\n",
        "\n",
        "for name, model in pipelines:\n",
        "  score = manual_cross_validation(df_ft, n, model, mode)\n",
        "  print(name,' : ', score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pv7SOPXHdD3"
      },
      "source": [
        "# manually tuning of our wonderful model (HGBR) and write name.csv for tuned model and untuned one\n",
        "\n",
        "train, test = train_test_split(df_ft, test_size=0.25, random_state=21)\n",
        "mode = 'development'\n",
        "train_ft_b, test_ft_b = ohe_beer(train, test, mode)\n",
        "x_train, y_train, x_test, y_test = drop_prepare(train_ft_b, test_ft_b, mode)\n",
        "\n",
        "# loss # least_squares is the choosen one? actually yeah\n",
        "loss = ['least_squares', 'least_absolute_deviation']\n",
        "models = [HistGradientBoostingRegressor(loss=d) for d in loss]\n",
        "train_results, test_results = model_score(x_train, y_train, x_test, y_test, models)\n",
        "plot_r2score(loss, 'loss', train_results, test_results)\n",
        "\n",
        "# learning rate # approximately 0.15\n",
        "learning_rate = np.linspace(0.05,1,10)\n",
        "models = [HistGradientBoostingRegressor(learning_rate=d, loss='least_squares') for d in learning_rate]\n",
        "train_results, test_results = model_score(x_train, y_train, x_test, y_test, models)\n",
        "plot_r2score(learning_rate, 'learning rate', train_results, test_results)\n",
        "\n",
        "# maximum iterations #75\n",
        "max_iter = [50,75,100,125,150]\n",
        "models = [HistGradientBoostingRegressor(max_iter=d, loss='least_squares', learning_rate=0.15) for d in max_iter]\n",
        "train_results, test_results = model_score(x_train, y_train, x_test, y_test, models)\n",
        "plot_r2score(max_iter, 'max iter', train_results, test_results)\n",
        "\n",
        "# L2 regularization # we leave it to zero\n",
        "l2_regularization = np.linspace(0,1,10)\n",
        "models = [HistGradientBoostingRegressor(l2_regularization=d, max_iter=75, loss='least_squares', learning_rate=0.15) for d in l2_regularization]\n",
        "train_results, test_results = model_score(x_train, y_train, x_test, y_test, models)\n",
        "plot_r2score(l2_regularization, 'L2 regularization', train_results, test_results)\n",
        "\n",
        "# scoring, apparently is the same\n",
        "scoring = ['loss', 'r2']\n",
        "models = [HistGradientBoostingRegressor(scoring=d,\n",
        "                                        l2_regularization=0, max_iter=80, loss='least_squares', learning_rate=0.15) for d in scoring]\n",
        "train_results, test_results = model_score(x_train, y_train, x_test, y_test, models)\n",
        "plot_r2score(scoring, 'scoring', train_results, test_results)\n",
        "\n",
        "# validation fraction, this can be fun! It was not, let it be default # 0.1\n",
        "validation_fraction = np.linspace(0.01,1,5)\n",
        "models = [HistGradientBoostingRegressor( validation_fraction=d,\n",
        "    scoring='r2', l2_regularization=0, max_iter=80, loss='least_squares', learning_rate=0.15) for d in validation_fraction]\n",
        "train_results, test_results = model_score(x_train, y_train, x_test, y_test, models)\n",
        "plot_r2score(validation_fraction, 'validation fraction', train_results, test_results)\n",
        "\n",
        "# file for model evaluation on platform manually tuned HGBR\n",
        "mode = 'evaluation'\n",
        "train_ft = df_ft\n",
        "test_ft = df_eval_ft\n",
        "train_ft_b, test_ft_b = ohe_beer(train_ft, test_ft, mode)\n",
        "x_train, y_train, x_test, _ = drop_prepare(train_ft_b, test_ft_b, mode)\n",
        "\n",
        "name = 'ManuallyTunedHist.csv'\n",
        "model = HistGradientBoostingRegressor(max_iter=80, loss='least_squares', learning_rate=0.15)\n",
        "x, y = train_model(x_train, y_train, x_test, model)\n",
        "write_submission(x, y, name)\n",
        "\n",
        "# file for model evaluation on platform HGBR before tuning\n",
        "model = HistGradientBoostingRegressor()\n",
        "name = 'baseHist.csv'\n",
        "x, y = train_model(x_train, y_train, x_test, model)\n",
        "write_submission(x, y, name)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}